%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out if you need a4paper
%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4 paper
\IEEEoverridecommandlockouts                              % This command is only needed if 
                                                          % you want to use the \thanks command
\overrideIEEEmargins                                      % Needed to meet printer requirements.

% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

% The following packages can be found on http:\\www.ctan.org
\usepackage{graphics} % for pdf, bitmapped graphics files
\usepackage{epsfig} % for postscript graphics files
\usepackage{mathptmx} % assumes new font selection scheme installed
\usepackage{times} % assumes new font selection scheme installed
\usepackage{amsmath} % assumes amsmath package installed
\usepackage{amssymb}  % assumes amsmath package installed
\usepackage{url}

\title{\LARGE \bf
Human Learning vs. Deep Learning for Atari Games
}

\author{Daniel Seita$^{1}$% <-this % stops a space
\thanks{$^{1}$University of California, Berkeley, USA. {\tt\small seita@berkeley.edu}}%
}

\begin{document}

\maketitle
\thispagestyle{empty}
\pagestyle{empty}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}

The combination of reinforcement learning and deep learning has led to impressive results in
training AI agents to play Atari games. Nonetheless, there is still much room for improvement,
especially for Atari games that require substantial long-term strategy, and the entire process of
how and what neural networks learn is poorly understood. In this paper, we complement the current
understanding of neural network based agents by collecting data from humans playing Atari games. 
We observe various quantitative and qualitative data from the human game play, and use the game logs
to develop a classifier that maps from game frames to human actions. Our results indicate that
humans learn a variety of ``optimal'' game strategies despite having difficulty with the game
controls, and that a convolutional neural network can successfully learn a mapping from frames to
actions despite noisy and highly imbalanced data.

\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}\label{sec:intro}

Deep learning has become one of the most popular and fastest growing research subfields of
artificial intelligence, as evident by the superior performance of (deep) neural networks over
classical methods in realms such as speech recognition~\cite{speech} and image
classification~\cite{image_classification}.

It has also been shown that deep learning can be used for challenging tasks in reinforcement
learning, where the job of an AI agent is not to perform ``simple'' classification, but to learn
from high-dimensional, correlated data with a scalar reward signal that is noisy and may exhibit
complicated, long-term rewards. For instance,~\cite{mnih-atari-2013} combined model-free
reinforcement learning with deep learning techniques to develop an AI agent capable of learning how
to play several Atari 2600 games at a level matching or exceeding human performance. The AI only
learned from the game frames and the score, just like how a human would learn.

Nonetheless, despite the progress advanced by neural networks, many questions still remain about how
exactly neural networks learn, and it is still unclear if this underlying ``process'' is at all
similar to the way that humans would
learn~\cite{szegedy2013intriguing,nguyen2015deep,goodfellow2014explaining}.  One way to bridge the
knowledge gap between the learning process of neural network agents and humans is to investigate how
humans would learn to play these Atari games. In this report, we investigate the numerous research
questions derived from human learning.

First, it is worthy to try and directly understand the human learning process via qualitative and
quantitative data. To do this, we design a task on Amazon Mechanical Turk, where players must play
three Atari games (Pong, Breakout, and Space Invaders) and answer well-designed survey questions.
Furthermore, we use different game orderings to test the possibility that transfer learning improves
game performance.

Second, it might be possible to augment the learning process of neural network agents with human
data to accelerate training to get fast, high-quality policies. We perform the first step of this
process by training a classifier to map from game frames to actions based on human data. The
long-term idea (not pursued here) is that one can incorporate the classifier during the exploration
phase of the neural network agent, when it is following a $(1-\epsilon)$-greedy policy with high
$\epsilon$ (i.e., close to one). Rather than have the ``$\epsilon$ cases'' correspond to
\emph{random} actions, the AI agent can use those cases to follow the \emph{human action}.
% Daniel: mention later that this might work because humans are exploring.

We report on the results of our survey and make high-level connections with the neural network based
agents, and also show that our trained convolutional neural network is reasonably skilled at
identifying human actions for given game frames. Ultimately, we hope to better understand the human
learning and deep learning processes that enable the corresponding agents to successfully play Atari
games.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Background and Related Work}\label{sec:related_work}

This report builds upon the groundbreaking work of~\cite{mnih-atari-2013} and the subsequent
follow-up~\cite{mnih-dqn-2015}. Both describe a neural network agent that can successfully
learn control policies for Atari 2600 games. The Arcade Learning
Environment~\cite{bellemare13arcade} provides the necessary software to test and benchmark agents on
Atari games, which are a relatively complicated reinforcement learning environment and sufficiently
diverse enough such that an agent that can play well in a variety of games is likely to have learned
without hand-engineered features.

To train the agent, they use a variant of Q-learning~\cite{Sutton_1998}. In standard Q-Learning for
solving a Markov Decision Process, one has state-action values $Q(s,a)$ for state $s$ and action
$a$. This is the expected sum of discounted rewards for the agent starting at state $s$, taking
action $a$, and from then on, playing optimally according to the action determined by the policy.
With Atari games, the states are \emph{sequences} of game frames $x_1,x_2,\ldots,x_t$ encountered
during game play\footnote{Technically,~\cite{mnih-atari-2013} reports that states are sequences of
game frames \emph{and} actions: $x_1,a_1,x_2,\ldots,a_t$. When doing Q-Learning, however, their code
only considers four consecutive frames and, to the best of our knowledge, does not take into account
actions other than the current one under consideration.}. The optimal action-value function $Q$
obeys the \emph{Bellman equation} identity: 

\begin{equation}\label{eq:bellman}
Q(s,a) = \mathbb{E}_{s'}\left[r + \gamma \cdot \max_{a'} Q(s',a') \mid s,a \right].
\end{equation}

The process of Q-Learning (or more generally, reinforcement learning) is to estimate the Q-values
using the Bellman equation as an iterative update.

The states are extremely high dimensional; even with downsampling, one frame is an $(84\times
84)$-dimensional input, and storing all $Q(s,a)$ values explicitly in a table is impractical.
Therefore, the $Q(s,a)$ values are \emph{approximated} by a neural network parameterized by its
weights $\theta$, and it is $\theta$ that the Q-Learning algorithm must learn.

In practice,~\cite{mnih-dqn-2015} uses a variant of online Q-Learning (with a $(1-\epsilon)$-greedy
policy for exploration) tailored to the task of Atari games.  The most important differences are:

\begin{itemize}
    \item They use \emph{experience replay} to store the agent's samples $(s_t,a_t,r_t,s_{t+1})$ at
    each time step in a dataset. When the Q-Learning algorithm updates $\theta$, it applies updates
    to a subset of the data. This is in contrast with standard Q-learning which only performs one
    update for each sample. Advantages include greater data efficiency and breaking the correlation
    among consecutive samples.
    \item They use a separate network for generating the targets (i.e., the $r + \gamma \cdot
    \max_{a'}Q(s',a')$ term in Equation~\ref{eq:bellman}), for the purposes of increasing the
    algorithm's numerical stability.
\end{itemize}

The DQN trained with this variant of Q-Learning was able to excel at many Atari games, especially
fast-paced games with simple rules such as Breakout. It was, however, weak on games such as
Montezuma's Revenge, which requires substantial long-term strategy.

There has been a surge of follow-up work for training agents to play Atari games.
In~\cite{nips-atari-2014}, they augment training using data collected \emph{offline} through the use
of Monte-Carlo tree search planning. The ``offline player,'' while substantially better than DQN,
cannot play in real time, but can be used to improve DQN's performance. The work
of~\cite{schaul2015prioritized} introduces prioritized experience replay to train DQN agents faster
since the most important transitions would be considered more frequently. This is a variant of
\emph{prioritized sweeping}~\cite{Moore93prioritizedsweeping}, a well-known technique used in
planning algorithms to pick the states to update in order of priority.
In~\cite{stadie2015incentivizing}, they propose more sophisticated exploration policies to improve
performance on Atari games.

In~\cite{wang2015dueling}, they present a different neural network architecture specialized for
reinforcement learning, and~\cite{van2015deep} proposes the Double DQN, which mitigates the problem
of the ``max'' operator using the same values to both select and evaluate an action (thus leading to
overoptimistic value estimates). At the time of publication, it was the highest-quality DQN
available. Finally, some follow-up work does not take a direct focus towards improving the baseline
DQN. For instance,~\cite{tampuu2015multiagent} extends the DQN architecture to \emph{multiagent
environments} and focuses on what happens when two independent DQN agents are trained for the game
of Pong.

While there has been much work concerning the technical aspects of DQN and its variants, there has
not been comparable interest in using data from human game logs. There is, however, a large
literature in the cognitive science field about human reasoning and
learning~\cite{stenninghuman,Friedenberg_Silverman_2006,NAP9853}.  The aim of this work is to bridge the gap
between cognitive science and DQN by analyzing human game logs and understanding the connection
between human learning versus deep learning.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Design of Experiments}\label{sec:experiments}

We present experiments that achieve the dual goals of (1) describing how humans learn to play Atari
games, and (2) using the human game log to develop a classifier that can map from game frames to
human actions.

\subsection{Amazon Mechanical Turk}\label{ssec:human_experiment}

\begin{figure}[t]
\centering
%\fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
\includegraphics[width=3.2in]{fig_atari_games}
\caption{The three Atari games we used in this experiment: Pong (left), Breakout (middle), and Space
Invaders (right). Pong and Breakout are similar in that both use a paddle to hit a ball that bounces
in the game space. In Space Invaders, the player controls the ship at the bottom and aims to shoot
at all the other ships.}
\label{fig:atari}
\end{figure}

To gather human data in a principled, programmatic way, we implemented a task for workers on Amazon
Mechanical Turk (AMTurk), which is an online service where people can do tasks in exchange for
payment. For our AMTurk task, we had players play three of the Atari games: Pong, Breakout, and
Space Invaders. Figure~\ref{fig:atari} shows example screenshots of the three respective games.

We selected these three for several reasons. First, they are all commonly used as benchmarks
for DQN agents; the original paper that introduced DQN used these three
games~\cite{mnih-atari-2013}. Second, Pong and Breakout are similar because players control a
paddle and can move it in one axis (vertically for Pong, horizontally for Breakout) and have to
hit a moving, bouncing ball. Both games require skillful coordination to hit the
ball at the correct locations and momentum to get the balls to go in desired directions.
Consequently, Pong and Breakout may test the effect of transfer learning, which we can evaluate by
checking the game performances of those players on Pong with or without Breakout experience (so long
as we have sufficient players to reduce the variance with few players). Finally, Space Invaders is a
game that DQN performs reasonably well on, but still often loses to human players, and requires
long-term strategy. It might be worthwhile to start exploring the question as to whether human data
can improve DQN's performance on Space Invaders.

For the AMTurk experiment, each player was assigned one of the six possible game orderings to 
evaluate transfer learning. The players played each game for exactly ten minutes. To motivate
the players to play and get high scores (and to avoid just letting the timer run out without doing
anything) we set three payment thresholds based off of performance. For each game, players earned
\$1.00, \$1.50, and \$2.00 for clearing the first, second, and third thresholds, respectively. All
thresholds were based off of game scores. For instance, Breakout's thresholds were set at 5, 30, and
90 points for one game episode. The first two thresholds are easy enough so that almost all new
players can reach them, while the third requires some additional skill and is attainable only for
fast learners or experienced players.

We told the players which controls to use (the four arrow keys, plus the space bar), but did not
tell them what they did, with the exception of telling them that the space bar in Breakout allowed
the ball to appear. We did this because a pilot run indicated that confusion among the space bar was
a top issue.

We also made the players answer various survey questions to collect qualitative data. We had a set
of post-game questions that players had to answer right after they played a game. The template was:

\begin{itemize}
    \item A Likert question: \emph{I played well.}
    \item A Likert question about a game-specific strategy:
    \begin{itemize}
        \item Pong: \emph{It is a good strategy to try and make the ball take an angled path towards the opponent.}
        \item Breakout: \emph{It is a good strategy to try and clear the board by eliminating one row at a time.}
        \item S.I.: \emph{It is a good strategy to try and eliminate ships in one column before moving on to another column.}
    \end{itemize}
    \item (Free resp.) \emph{What was the final strategy you decided on?}
    \item (Free resp.) \emph{Was there anything from a previous game in this HIT\footnote{A HIT in AMTurk stands
    for ``Human Intelligence Task.''} that helped?}
\end{itemize}

The purpose of the game-specific Likert scale question is to get the players to think about good
general strategies for that game and also to make it easier for them to answer the free response
questions. All Likert questions were on a scale of five: strongly disagree, disagree, undecided,
agree, strongly agree.

This template of post-game survey questions was asked directly after a game, so players did the
following in succession: Game 1, post-game survey, Game 2, post-game survey, Game 3, post-game
survey, and then finalized their task with some general survey questions at the end. At the very
end, we ask the players if they think they now knew effective strategies for all three games
(True/False scale) and how they would rank their level of expertise in each game (novice,
intermediate, expert).  Our final question explicitly asks them if they thought they had developed
good strategies to play all three games. We discuss our results in Section~\ref{ssec:human_results}.

\subsection{Neural Network Classifier}\label{ssec:nn_experiment}

In active reinforcement learning, an \emph{exploration policy} is necessary because the agent needs
to explore the state space to learn an optimal policy. While it does so, it balances exploration
(trying new actions to reach new states) with exploitation (taking optimal actions according to
estimated state-action values). As reported in~\cite{mnih-dqn-2015}, the DQN algorithm follows a
$(1-\epsilon)$-greedy policy during training. This means the agent selects actions at random with
probability $\epsilon$, and follows the optimal action with probability $1-\epsilon$. Their
algorithm initialized $\epsilon=1$, and gradually reduced it to 0.1 over the first one million
frames, and then fixed it at 0.1 thereafter. The reason for high $\epsilon$ at the beginning is so
that the agent can sufficiently explore the state space, which is a requirement for Q-learning to
converge to the optimal policy.

Humans, however, do not generally perform random actions when first starting to play a game (i.e.,
during the ``exploration phase''). It would therefore be interesting to see what would happen to the
DQN agent during training if the $\epsilon$ cases correspond to actions that a \emph{human} would
most likely take (rather than being a random action). If a human can quickly learn how to play a
game well, then the actions he or she takes might help a DQN agent to converge to optimal policies
faster.

To pick an action that a human would likely take, we train a classifier that takes a downsampled
game frame as input and provides a ranking of actions as output (we only work with the highest
probability action). We use convolutional neural networks (CNNs) as our classifier due to their
strong performance on other image classification tasks.  The game log from the AMTurk experiments
provide the training data since it provides a record of all game frames encountered, along with the
corresponding keystrokes from the player. The CNN we design is based on the CNN
from~\cite{mnih-atari-2013}. It takes an $(84\times 84)$ grayscale image as input. The first hidden
layer convolves 16 $8\times 8$ kernels with stride size 4, then applies max-pooling and the
rectifier nonlinearity. The second hidden layer convolves 32 $4\times 4$ kernels with stride size 2,
then applies max-pooling and the rectifier nonlinearity. The third (and final) hidden layer is fully
connected and has 256 rectifier units. The output layer is a fully connected linear layer with a
single output for each valid action; the number of valid actions varies depending on the game.

Due to time and resource constraints, we only train a CNN for recognizing human actions from game
frames of the Space Invaders game, leaving Breakout and Pong frame-action classification for future
work. Space Invaders has six actions: nothing, fire, move-left, move-right, fire-move-left, and
fire-move-right. We describe our results in Section~\ref{ssec:nn_results}. We did not use this
classifier for training DQN, again due to time constraints, but our classifier is publicly available
\footnote{\url{https://github.com/DanielTakeshi/CS-287-Final-Project}} and it is straightforward to
incorporate it in Nathan Sprague's DQN
implementation\footnote{\url{https://github.com/spragunr/deep_q_rl}}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Results}\label{sec:results}

\subsection{Human Experiment Results}\label{ssec:human_results}


\begin{figure}[t]
\centering
\includegraphics[width=3.2in]{fig_boxplots_human_results.png}
\caption{This displays boxplots of our 34 human players' results on the three games, with scores on
the $y$-axis. The boxes cover the 25th through 75th percentiles, with the median indicated by the
line in the box, and the whiskers go out to the minimum of the maximum element, or 1.5 times the
IQR of the data. The red dot is the \emph{average} DQN score for that game, and the yellow dot is
the author's \emph{average} score. Among all three plots, there are only three outliers. One is from
Pong, where one player lost 21-18 and therefore scored -3, and the other two are from Space
Invaders, where two different players scored 4370 and 7015 (!) points.}
\label{fig:human_results}
\end{figure}

For our AMTurk experiment, our goal is to eventually have 120 total responses, with 20 players for
each of the six valid game orderings to test transfer learning. Due to constraints from our server,
which cannot handle many players using it, we had to deploy the AMTurk hit in small batches and only
obtained 34 responses.

We first report the players' raw scores. For simplicity, only the \emph{highest} score the players
got in their ten minutes for a game are considered. Figure~\ref{fig:human_results} presents three
boxplots of all the high scores. We augment these plots with the \emph{average} DQN score
(from~\cite{mnih-dqn-2015}), and the \emph{average} score of the first author. Note that for Pong,
the player plays a computer player and the game ends when one of the two gets 21 points. The score
for Pong used in this paper is the player's score minus the computer player's score, so if the human
lost $21-6$ the score we use is $-15$.

There are several immediate observations. First, Pong was especially difficult for our workers, and
not a single player ended up beating the computer; the closest was a $21-18$ loss. In contrast, DQN
performs very well (typically winning $19-0$ or $20-0$) and the author noticed a bug in the game
where, due to various repeated patterns, one can park the player's paddle in a certain spot in the
board, and the computer player will always hit the ball towards the edge of the human players'
paddle, resulting in the ball bouncing across the board and past the computer.

Breakout was also challenging, as it requires good coordination skills with using the paddle. (This
explains why DQN is so much better than all human players in this game, since coordination is not
generally an issue with computer players.) A common complaint about Pong and Breakout was that the
controls were too difficult, since players not only had to get used to the controls, but also had to
learn how to hit balls in a way so that they reach their desired destination.

Controls were less of an issue with Space Invaders, and our workers did relatively well compared to
DQN (and the first author). A considerable fraction of our players exceeded DQN's performance in
their ten minutes, with someone even getting a score of 7015.

\begin{figure}[t]
\centering
\includegraphics[width=3.2in]{fig_boxplots_transfer_learning}
\caption{This displays boxplots of humans playing Pong and Breakout with various orderings to test
the effect of transfer learning. Legend: ``Pong'' means playing Pong without having previously
played Breakout, ``P.a.B'' means playing Pong (a)fter (B)reakout, ``Breakout'' means playing
Breakout without having previously played Pong, and ``B.a.P.'' means playing Breakout (a)fter
(P)ong. The two Pong plots are scaled so that they have the same $y$-axis to facilitate comparison;
a similar case holds for the Breakout plots. Note that for these results, the time when the players
played Space Invaders is irrelevant.}
\label{fig:human_transfer}
\end{figure}

We next discuss the impact, if any, of transfer learning. Since Space Invaders is very different
from Pong and Breakout, we only investigate the performance of players based on different orderings
of playing Pong and Breakout. Figure~\ref{fig:human_transfer} shows four box plots of players' high
scores on Pong and Breakout using four different cases: playing Pong without Breakout experience,
playing Pong with Breakout experience, playing Breakout without Pong experience, and playing
Breakout with Pong experience.

The plots show that the effect of transfer learning is limited, which is not surprising since each
plot only represented 17 data points. The results we do have suggest that players do better in Pong
after Breakout experience, but that players perform \emph{worse} in Breakout despite Pong
experience.

Next, we report on other statistics we collected. Table~\ref{tab:human_results} concisely represents
seven of our five-point Likert scale questions, and for each one, the number of players who selected
a certain answer (from ``Strongly Disagree'' to ``Strongly Agree''). The ``Exp.'' represents the
Likert question of \emph{I have a lot of gaming experience}. The others are from
Section~\ref{ssec:human_experiment}: ``P1'' and ``P2'' represent \emph{Pong} and ask \emph{I played
well} and the game-specific question respectively (for Pong, it was about hitting the ball at an
angle). A similar case holds for ``B1'' and ``B2'' for Breakout, and then ``SI1'' and ``SI2'' for
Space Invaders.

The ``Exp.'' row shows that more than half of our players agreed that they had lots of gaming
experience, which makes sense due to the nature of our hit, and the fact that players who are
extremely bad at these games would likely not finish the HIT.

The answers for the game-specific questions are particularly interesting for Breakout, which asked
the player to agree with the following statement: \emph{It is a good strategy to try and clear the
board by eliminating one row at a time.} This is actually \emph{not} the optimal strategy; the
easiest way to get lots of points is to break through a \emph{column} through the rows to get the
ball to hit the back rows. The ``B2'' line shows that there was no consensus answer on this among
the players, which is probably due to how most players never experienced that ``breakout'' case
happening.

In terms of the \emph{I played well} questions, players generally felt that they did not do well in
Pong or Breakout, which had $13+9$ and $11+7$ people answer some form of disagree, respectively.
Space Invaders was a different story; $14+11$ people agreed with the fact that they played well.

\begin{table}[t]
\centering
\begin{tabular}{|c||c|c|c|c|c|}
\hline
  & S. Dis. & Dis. & Undecided & Agree & S. Agree \\
\hline
Exp. & 1  & 4 & 2 & 9 & 18  \\
\hline
P1 & 13 & 9 & 3 & 7 & 2 \\
P2 & 1 & 2 & 3 & 11 & 17 \\
\hline
B1 & 11 & 7 & 7 & 7 & 2 \\
B2 & 8 &  7 & 9 & 6 & 4 \\
\hline
SI1 & 2 & 4 & 3 & 14 & 11 \\
SI2 & 4 & 7 & 3 & 12 & 8 \\
\hline
\end{tabular}
\caption{Quantitative results on seven different Likert-scale questions, as detailed in the text.
Note that ``S.'' is short for ``Strongly'' and ``Dis.'' is short for ``Disagree.''}
\label{tab:human_results}
\end{table}

The reason why players felt they did not do well in Pong or Breakout can be attributed to
difficulties in using the game controls, which were a common theme of discussion in the free
response questions. We now present a sample of some of those responses at different points in the
survey. All responses are non-edited and any grammatical errors are due to the players.

After Pong, two players said:

\begin{itemize}
    \item \emph{my strategy during this game was an utter disaster throughout the entire
    phase of play.}
    \item \emph{Breakout helped learn how to hit the ball at an angle}
\end{itemize}

The second comment hints at the possibility of transfer learning.

After Breakout, two players said:

\begin{itemize}
    \item \emph{The best way to clear the board is by clearing one column, and getting the ball to bounce
   around from the top down.  I could not implement this strategy because of the controls.} 
    \item \emph{The game was difficult and the slider was too slow for the ball. Not to mention that
    it was difficult to control the bounce of the ball and control where it goes so you could
    eliminate one row at a time.}
\end{itemize}

Note that the second player (incorrectly) disagrees with the first on the recommended strategy.

After Space Invaders, two players said:

\begin{itemize}
    \item \emph{Eliminate most off the lowest aliens first, then concentrate on an outer column}
    \item \emph{I really liked this game as a kid. I did the lower levels first if I could but at
    the very start I tried to get rid of a column.}
\end{itemize}

With Space Invaders, we received more strategy-related comments rather than game control complaints.
Note that the first player recommends the columns after eliminating the lower ships (not aliens),
but the second seems to suggest that it's best to eliminate a column right at the beginning.

After all three games, we also asked players a post-game survey question as to whether they felt
that an earlier game had helped them with a late game. Two players said:

\begin{itemize}
    \item \emph{None of the games seemed related enough where I would need to use info from one of
    them on the others.}
    \item \emph{If there were a way to have better contorl of the paddle, I would try to
    hit more angular shots in the PONG game. The Atari paddle controller on the old 2600 system made
    this easily possible. The keyboard controls show weakness [...]}
\end{itemize}

The second player's comment has been cut off for space constraints. Incidentally, that player was
the one who scored 7015 on Space Invaders.

Overall, we believe that our players had a reasonable understanding of how to play the games given
the time constraints, but the major difficulty is clearly getting used to the controls.

\subsection{Neural Network Experiment Results}\label{ssec:nn_results}

\begin{table}[t]
\centering
\begin{tabular}{|c||c|c|c|c|c|c|}
\hline
  & 0 & 1 & 2 & 3 & 4 & 5 \\
\hline
Train & 199934 & 176589 & 80665 & 77749 & 18299 & 20340 \\
Test  & 49927  & 44354 & 19910 & 19663 & 4548 & 5094 \\
\hline
\end{tabular}
\caption{Distribution of actions for the Space Invaders train/test data. Key: 0 = no action, 1 =
fire (not moving), 2 = move right, 3 = move left, 4 = fire move right, 5 = fire move left.}
\label{tab:data_distribution}
\end{table}

We build and train a CNN as described in Section~\ref{ssec:nn_experiment} using CAFFE~\cite{caffe}
on logs from 34 cases of Space Invaders. Our game logs recorded all changes in keystrokes (based on
arrow keys and the space bar) and at what frames these occurred throughout the game. We also develop
an image extraction pipeline that can determine the frames the players saw during each game. Using
that information, we can assign for each frame, the Space Invaders action a particular player chose.

Space Invaders has six actions, as described in Section~\ref{ssec:nn_experiment}.   As expected, the
action ``0'' (corresponding to doing nothing) composed the vast majority of the actions we computed.
To make our data distribution more balanced, we performed a simple preprocessing step to randomly
eliminate half of the ``zero'' cases from the data. The resulting distribution of actions for the
training and testing data is shown in Table~\ref{tab:data_distribution}.  To get the training and
testing data, we randomly allocate the full data into training/testing batches so that 80\% of the
data is in training.

All of our (grayscale) frames were downsampled to $(84\times 84)$. Most players, when playing ten
minutes of Space Invaders, went through about 35,000 frames, with the exact quantity varying due to
how some players elected to do nothing after they played a few episodes but before the 10 minutes
were up (in this case, our log did not track their actions). All together, the training and testing
data were 16.3GB.

We trained our network on a workstation machine with an NVIDIA GeForce Titan X GPU for 100k
iterations using momentum stochastic gradient descent. After 100k iterations, our CNN obtained
60.74\% accuracy. Note that after 10k, 25k, and 50k iterations (for example), the CNN had 50.66\%,
54.56\%, and 58.9\% accuracy, respectively, perhaps suggesting that minor improvements are possible
with additional training. That the classifier is able to classify this many images correctly is
remarkable due to the imbalanced distribution of our actions.

Despite the impressive accuracy of our classifier, there are some obvious steps one might take to
improve it beyond (or instead of) our one preprocessing step. Some extensions include:

\begin{itemize}
    \item One can use $(84\times 84\times k)$ input where $k > 1$ is a small integer (e.g., 4 or 5).
    This might result in more meaningful classification since choosing a reasonable action often
    depend on knowing a \emph{sequence} of frames.
    \item One can get rid of the beginning and ending frames for each game. Many players did not do
    any actions for the first few frames, and several ignored our instructions and did not play for
    all ten minutes. This would also help to eliminate many of our ``0'' cases.
    \item One can incorporate a delay of a few frames; upon seeing a frame at time $t$, one could
    instead choose the action that was selected on frame $t+n$ for a small, positive integer $n$.
    This might make sense because if players see a frame, delays in reaction time will result in the
    keystroke happening a few frames later.
\end{itemize}

Due to lack of computational power at our disposal, we were unable to run the experiments for as
long as we wished, though we have made the trained caffe model file publicly available for reuse at
a later date.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusions and Future Work}\label{sec:conclusions}

We conducted an experiment on Amazon Mechanical Turk to augment the current understanding of human
learning versus deep learning. We identified that humans had great difficulty playing games that
require experience with controls (Pong and Breakout) but were able to think and learn how to play
Space Invaders quickly enough so that, even after just ten minutes, the performance is comparable to
DQN.

We also trained a CNN to map from game frames to human actions, where the data comes from the game
logs on Amazon Mechanical Turk. Our classifier got over 60 percent accuracy despite challenges
from our high-dimensional, noisy, and imbalanced data.

Straightforward directions for future work include continuing the Amazon Mechanical Turk experiments
on more human participants, testing on other Atari games, performing some of the extensions of the
CNN classifier as discussed in Section~\ref{ssec:nn_results}, and implementing this classifier into
action in a DQN to improve the exploration policy.

Some more elaborate extensions of this work might involve running Q-Learning on the human data
itself to see if the Q-Learner's policy is as good as what DQN learned, and investigating inverse
reinforcement learning to propose a pseudo-reward function that explains people's behavior in their
exploration stages.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Acknowledgments}

We thank Andrew Liu for setting up the Amazon Mechanical Turk experiment.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\addtolength{\textheight}{-12cm}
% This command serves to balance the column lengths on the last page of the document manually. It
% shortens the textheight of the last page by a suitable amount.  This command does not take effect
% until the next page so it should come on the page before the last. Make sure that you do not
% shorten the textheight too much.

\bibliographystyle{IEEEtran}
\bibliography{Daniel_Seita_Final_Paper_CS287}

% Daniel: This was their old table code.
%\begin{table}[h]
%\caption{An Example of a Table}
%\label{table_example}
%\begin{center}
%\begin{tabular}{|c||c|}
%\hline
%One & Two\\
%\hline
%Three & Four\\
%\hline
%\end{tabular}
%\end{center}
%\end{table}

% Daniel: This was their old figure code.
%\begin{figure}[thpb]
%\centering
%\framebox{\parbox{3in}{We suggest that you use a text box to insert a graphic (which is ideally a
%300 dpi TIFF or EPS file, with all fonts embedded) because, in an document, this method is somewhat
%more stable than directly inserting a picture.  }}
%%\includegraphics[scale=1.0]{figurefile}
%\caption{Inductance of oscillation winding on amorphous
%magnetic core versus DC bias magnetic field}
%\label{figurelabel}
%\end{figure}


\end{document}
